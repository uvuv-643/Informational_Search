{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b86b79-7a24-4519-8ffd-a5307ecddeb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T14:57:32.879815Z",
     "iopub.status.busy": "2025-11-03T14:57:32.878744Z",
     "iopub.status.idle": "2025-11-03T14:57:39.472764Z",
     "shell.execute_reply": "2025-11-03T14:57:39.472005Z",
     "shell.execute_reply.started": "2025-11-03T14:57:32.879787Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from splade.splade.models.transformer_rep import Splade\n",
    "\n",
    "df = load_dataset(\"microsoft/ms_marco\", \"v1.1\")\n",
    "\n",
    "model_type_or_dir = \"naver/splade-cocondenser-ensembledistil\"\n",
    "\n",
    "model = Splade(model_type_or_dir, agg=\"max\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type_or_dir)\n",
    "reverse_voc = {v: k for k, v in tokenizer.vocab.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d5fad-a386-454e-a420-90b46a6bf0d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T14:57:48.797149Z",
     "iopub.status.busy": "2025-11-03T14:57:48.796490Z",
     "iopub.status.idle": "2025-11-03T15:33:47.736896Z",
     "shell.execute_reply": "2025-11-03T15:33:47.735919Z",
     "shell.execute_reply.started": "2025-11-03T14:57:48.797126Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5800030it [35:58, 2686.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backup saved at 20251103_153037 - 1524032 documents processed\n",
      "\n",
      "Processing complete. Total documents processed: 1524032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "os.makedirs(\"backups\", exist_ok=True)\n",
    "\n",
    "reverse_index = defaultdict(list)\n",
    "batch_size = 32\n",
    "\n",
    "last_save_time = time.time()\n",
    "save_interval = 30 * 600\n",
    "counter = 0\n",
    "with open(\"collection.tsv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    \n",
    "    batch_docs = []\n",
    "    batch_ids = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    for row in tqdm(rd):\n",
    "        counter += 1\n",
    "        if counter < 4_276_000:\n",
    "            continue\n",
    "            \n",
    "        batch_ids.append(row[0])\n",
    "        batch_docs.append(row[1])\n",
    "        \n",
    "        if len(batch_docs) == batch_size:\n",
    "            passage_tokens = tokenizer(batch_docs, return_tensors=\"pt\", truncation=True, \n",
    "                                      max_length=512, padding=True).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_reps = model(d_kwargs=passage_tokens)[\"d_rep\"]\n",
    "            \n",
    "            for i, (doc_id, doc_rep) in enumerate(zip(batch_ids, batch_reps)):\n",
    "                doc_rep = doc_rep.squeeze()\n",
    "                mask = doc_rep > 0.01\n",
    "                indices = torch.arange(doc_rep.size(0), device=device)[mask]\n",
    "                weights = doc_rep[mask]\n",
    "                \n",
    "                sorted_indices = weights.argsort(descending=True)\n",
    "                indices = indices[sorted_indices].cpu().numpy()\n",
    "                weights = weights[sorted_indices].cpu().numpy()\n",
    "                \n",
    "                for idx, weight in zip(indices, weights):\n",
    "                    reverse_index[reverse_voc[idx]].append((doc_id, float(weight)))\n",
    "            \n",
    "            total_processed += len(batch_docs)\n",
    "            batch_docs = []\n",
    "            batch_ids = []\n",
    "            \n",
    "            current_time = time.time()\n",
    "            if current_time - last_save_time >= save_interval or counter > 5_800_000:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                \n",
    "                with open(f\"backups/reverse_index_{timestamp}.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(dict(reverse_index), f)\n",
    "                \n",
    "                with open(f\"backups/progress_{timestamp}.txt\", \"w\") as f:\n",
    "                    f.write(f\"Documents processed: {total_processed}\\n\")\n",
    "                    f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                \n",
    "                print(f\"\\nBackup saved at {timestamp} - {total_processed} documents processed\")\n",
    "                last_save_time = current_time\n",
    "                break\n",
    "    \n",
    "    \n",
    "print(f\"\\nProcessing complete. Total documents processed: {total_processed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8e4bf-14cf-4a74-9607-6aad323515fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
