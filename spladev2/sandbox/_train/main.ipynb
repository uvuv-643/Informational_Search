{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eecd8457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2088, 2403, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "reverse_voc = {v: k for k, v in tokenizer.vocab.items()}\n",
    "\n",
    "tokenizer\n",
    "\n",
    "tokenizer('hello world  14  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "93617e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 477/477 [01:05<00:00,  7.24it/s]\n",
      "100%|██████████| 477/477 [01:03<00:00,  7.51it/s]\n",
      "100%|██████████| 477/477 [01:04<00:00,  7.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BertTokenClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', hidden_dim=None, dropout=0.3):\n",
    "        super(BertTokenClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        bert_hidden_size = self.bert.config.hidden_size  # 768 для bert-base\n",
    "        \n",
    "        # Если hidden_dim не указан, используем половину размера BERT\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = bert_hidden_size // 2  # 384\n",
    "        \n",
    "        # Первый полносвязный слой: BERT hidden_size -> hidden_dim\n",
    "        self.fc1 = nn.Linear(bert_hidden_size, hidden_dim)\n",
    "        \n",
    "        # Второй полносвязный слой: hidden_dim -> 1\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Dropout для регуляризации\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Функция активации\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state  # (batch_size, seq_len, 768)\n",
    "        \n",
    "        # Первый слой с активацией\n",
    "        x = self.fc1(token_embeddings)  # (batch_size, seq_len, hidden_dim)\n",
    "        x = self.relu(x)  # ReLU активация\n",
    "        x = self.dropout(x)  # Dropout для регуляризации\n",
    "        \n",
    "        # Второй слой (финальный классификатор)\n",
    "        logits = self.fc2(x)  # (batch_size, seq_len, 1)\n",
    "        \n",
    "        return logits.squeeze(-1)  # (batch_size, seq_len)\n",
    "\n",
    "model = BertTokenClassifier(hidden_dim=384, dropout=0.3)\n",
    "\n",
    "# ВАЖНО: теперь нужно обучать параметры всех слоев классификатора\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.fc1.parameters()},\n",
    "    {'params': model.fc2.parameters()}\n",
    "], lr=2e-5)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "reverse_voc = {v: k for k, v in tokenizer.vocab.items()}\n",
    "cls_id = tokenizer.cls_token_id  # 101\n",
    "sep_id = tokenizer.sep_token_id  # 102\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    all_token_ids = list(tokenizer.vocab.values())\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_token_ids), batch_size)):\n",
    "        batch_ids = all_token_ids[i:i+batch_size]\n",
    "        \n",
    "        # Каждый токен оборачиваем в [CLS] token [SEP]\n",
    "        input_ids_list = [[cls_id, tid, sep_id] for tid in batch_ids]\n",
    "        attention_mask_list = [[1, 1, 1] for _ in batch_ids]\n",
    "        \n",
    "        # Паддинг до одинаковой длины (уже одинаковые, но для безопасности)\n",
    "        max_len = 3\n",
    "        input_ids_padded = []\n",
    "        attention_mask_padded = []\n",
    "        for seq, mask in zip(input_ids_list, attention_mask_list):\n",
    "            input_ids_padded.append(seq + [0] * (max_len - len(seq)))\n",
    "            attention_mask_padded.append(mask + [0] * (max_len - len(mask)))\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids_padded).to(device)  # (batch_size, 3)\n",
    "        attention_mask = torch.tensor(attention_mask_padded).to(device)  # (batch_size, 3)\n",
    "        \n",
    "        # Labels для каждого токена в последовательности (нам нужен только токен на позиции 1)\n",
    "        labels = torch.tensor([1.0 if 'tech' in reverse_voc[tid] else 0.0 for tid in batch_ids]).to(device)  # (batch_size,)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)  # (batch_size, 3)\n",
    "        \n",
    "        # Берем предсказания только для токена на позиции 1 (наш целевой токен)\n",
    "        token_predictions = outputs[:, 1]  # (batch_size,)\n",
    "        \n",
    "        loss = criterion(token_predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ac6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panic\n",
      "hispanic\n",
      "panicked\n",
      "Token IDs: [6634, 6696, 16035]\n",
      "Probabilities: [0.00035994 0.00032089 0.00165672]\n",
      "Predicted toxic (1=toxic): [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = []\n",
    "    for i in range(30000):\n",
    "        if 'panic' in reverse_voc[i]:\n",
    "            print(reverse_voc[i])\n",
    "            x.append(i)\n",
    "    \n",
    "    test_token_ids = x\n",
    "    \n",
    "    # Формируем входные данные правильно\n",
    "    input_ids_list = [[cls_id, tid, sep_id] for tid in test_token_ids]\n",
    "    attention_mask_list = [[1, 1, 1] for _ in test_token_ids]\n",
    "    \n",
    "    test_input_ids = torch.tensor(input_ids_list).to(device)  # (num_tokens, 3)\n",
    "    test_attention_mask = torch.tensor(attention_mask_list).to(device)  # (num_tokens, 3)\n",
    "    \n",
    "    prediction_logits = model(test_input_ids, attention_mask=test_attention_mask)  # (num_tokens, 3)\n",
    "    \n",
    "    # Берем предсказания для токена на позиции 1\n",
    "    token_logits = prediction_logits[:, 1]  # (num_tokens,)\n",
    "    prediction_probs = torch.sigmoid(token_logits)\n",
    "    \n",
    "    print(\"Token IDs:\", test_token_ids)\n",
    "    print(\"Probabilities:\", prediction_probs.cpu().numpy())\n",
    "    predicted_labels = (prediction_probs > 0.5).long()\n",
    "    print(\"Predicted toxic (1=toxic):\", predicted_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7c3253a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2273"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reverse_voc.keys())[:1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef10af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
