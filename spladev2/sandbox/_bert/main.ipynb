{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11a85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\n",
    "    task=\"fill-mask\",\n",
    "    model=\"google-bert/bert-base-uncased\",\n",
    "    dtype=torch.float16,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ea4d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0728759765625,\n",
       "  'token': 9701,\n",
       "  'token_str': 'ghana',\n",
       "  'sequence': 'itmo is the university in ghana.'},\n",
       " {'score': 0.035247802734375,\n",
       "  'token': 16274,\n",
       "  'token_str': 'mozambique',\n",
       "  'sequence': 'itmo is the university in mozambique.'},\n",
       " {'score': 0.0341796875,\n",
       "  'token': 11959,\n",
       "  'token_str': 'tanzania',\n",
       "  'sequence': 'itmo is the university in tanzania.'},\n",
       " {'score': 0.0311126708984375,\n",
       "  'token': 16878,\n",
       "  'token_str': 'macau',\n",
       "  'sequence': 'itmo is the university in macau.'},\n",
       " {'score': 0.0213775634765625,\n",
       "  'token': 10031,\n",
       "  'token_str': 'uganda',\n",
       "  'sequence': 'itmo is the university in uganda.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"ITMO is the university in [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4eb8baad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 30522])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"This movie is [MASK]!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ce077bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12476,  2307,  6429,  9788,  4121, 10392,  4689,  9951,  2204,  8235])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_id = tokenizer.mask_token_id\n",
    "mask_position = (inputs['input_ids'] == mask_token_id).nonzero(as_tuple=True)[1][0]\n",
    "\n",
    "probs = torch.softmax(outputs.logits[0][mask_position], dim=-1)\n",
    "\n",
    "torch.topk(probs, top_k).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c0625234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predictions:\n",
      "1. awesome         (index: 12476) - 0.1405\n",
      "2. great           (index:  2307) - 0.0885\n",
      "3. amazing         (index:  6429) - 0.0874\n",
      "4. incredible      (index:  9788) - 0.0371\n",
      "5. huge            (index:  4121) - 0.0292\n",
      "6. fantastic       (index: 10392) - 0.0274\n",
      "7. crazy           (index:  4689) - 0.0258\n",
      "8. ridiculous      (index:  9951) - 0.0174\n",
      "9. good            (index:  2204) - 0.0161\n",
      "10. brilliant       (index:  8235) - 0.0158\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "top_indices = torch.topk(probs, top_k).indices\n",
    "\n",
    "print(\"Top predictions:\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    token = tokenizer.decode(idx)\n",
    "    prob = probs[idx].item()\n",
    "    print(f\"{i+1}. {token:15} (index: {idx:5}) - {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f737d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 101, 2023, 3185, 2003,  103,  999,  102]])\n",
      "Token IDs: [101, 2023, 3185, 2003, 103, 999, 102]\n",
      "Tokens: ['[CLS]', 'this', 'movie', 'is', '[MASK]', '!', '[SEP]']\n",
      "Decoded: [CLS] this movie is [MASK]! [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs:\", inputs['input_ids'])\n",
    "print(\"Token IDs:\", inputs['input_ids'][0].tolist())\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "print(\"Decoded:\", tokenizer.decode(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da639d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31b6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
