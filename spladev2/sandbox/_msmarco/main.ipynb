{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836f8468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from datasets) (2.3.4)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: packaging in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: certifi in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.1.10)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1.0.0->datasets)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading aiohttp-3.13.2-cp312-cp312-macosx_11_0_arm64.whl (491 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl (94 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl (50 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl (47 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-macosx_12_0_arm64.whl (34.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, sniffio, pyarrow, propcache, multidict, h11, frozenlist, dill, attrs, aiohappyeyeballs, yarl, pandas, multiprocess, httpcore, anyio, aiosignal, httpx, aiohttp, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/21\u001b[0m [datasets]/21\u001b[0m [datasets]ess]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.11.0 attrs-25.4.0 datasets-4.3.0 dill-0.4.0 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 multidict-6.7.0 multiprocess-0.70.16 pandas-2.3.3 propcache-0.4.1 pyarrow-22.0.0 pytz-2025.2 sniffio-1.3.1 tzdata-2025.2 xxhash-3.6.0 yarl-1.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cb9fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "df = load_dataset(\"microsoft/ms_marco\", \"v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379dbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " 'passage_text': [\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "  \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "  'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ',\n",
       "  'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       "  'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       "  'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.',\n",
       "  'RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       "  'vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.',\n",
       "  'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       "  'Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. '],\n",
       " 'url': ['https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "  'https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "  'http://acronyms.thefreedictionary.com/RBA',\n",
       "  'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "  'http://rba-africa.com/about/what-is-rba/',\n",
       "  'http://resultsleadership.org/what-is-results-based-accountability-rba/',\n",
       "  'http://rba-africa.com/about/what-is-rba/',\n",
       "  'http://searchsecurity.techtarget.com/definition/risk-based-authentication-RBA',\n",
       "  'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "  'http://www.rbaconsulting.com/']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train']['passages'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bfc0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=/Users/uvuv/Informational_Search\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONPATH=/Users/uvuv/Informational_Search\n",
    "if os.environ[\"PYTHONPATH\"] not in sys.path:\n",
    "    sys.path.insert(0, os.environ[\"PYTHONPATH\"])\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from spladev2.cloned.splade.splade.models.transformer_rep import Splade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "791486ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type_or_dir = \"naver/splade-cocondenser-ensembledistil\"\n",
    "\n",
    "model = Splade(model_type_or_dir, agg=\"max\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type_or_dir)\n",
    "reverse_voc = {v: k for k, v in tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04261551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:08,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6917])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "mrr10 = 0\n",
    "for index, passage in tqdm(enumerate(df['train']['passages'][:100])):\n",
    "    passge_text: List[str] = passage['passage_text']\n",
    "    query: str = df['train']['query'][index]\n",
    "    with torch.no_grad():\n",
    "        query_rep = model(d_kwargs=tokenizer(query, return_tensors=\"pt\"))[\"d_rep\"].squeeze()\n",
    "        doc_rep = torch.empty(0)\n",
    "        for text in passge_text:\n",
    "            doc_rep = torch.cat([doc_rep, model(d_kwargs=tokenizer(text, return_tensors=\"pt\"))[\"d_rep\"]])\n",
    "\n",
    "        relevant_indicies = torch.argsort(query_rep @ doc_rep.T, descending=True)\n",
    "        result_index = torch.argmax(torch.tensor(df['train']['passages'][index]['is_selected']))\n",
    "        rank = torch.where(relevant_indicies == result_index)\n",
    "        mrr10 += 1 / (rank[0] + 1)\n",
    "print(mrr10 / 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6c04854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ir_datasets\n",
      "  Downloading ir_datasets-0.5.11-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting beautifulsoup4>=4.4.1 (from ir_datasets)\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting inscriptis>=2.2.0 (from ir_datasets)\n",
      "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting lxml>=4.5.2 (from ir_datasets)\n",
      "  Downloading lxml-6.0.2-cp312-cp312-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from ir_datasets) (2.3.4)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from ir_datasets) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.22.0 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from ir_datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.38.0 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from ir_datasets) (4.67.1)\n",
      "Collecting trec-car-tools>=2.5.4 (from ir_datasets)\n",
      "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
      "Collecting lz4>=3.1.10 (from ir_datasets)\n",
      "  Downloading lz4-4.4.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting warc3-wet>=0.2.3 (from ir_datasets)\n",
      "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting warc3-wet-clueweb09>=0.2.5 (from ir_datasets)\n",
      "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting zlib-state>=0.1.3 (from ir_datasets)\n",
      "  Downloading zlib_state-0.1.10.tar.gz (9.6 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ijson>=3.1.3 (from ir_datasets)\n",
      "  Downloading ijson-3.4.0.post0-cp312-cp312-macosx_11_0_arm64.whl.metadata (23 kB)\n",
      "Collecting unlzw3>=0.2.1 (from ir_datasets)\n",
      "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: pyarrow>=16.1.0 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from ir_datasets) (22.0.0)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4>=4.4.1->ir_datasets)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from beautifulsoup4>=4.4.1->ir_datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (2025.10.5)\n",
      "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir_datasets)\n",
      "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading ir_datasets-0.5.11-py3-none-any.whl (866 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.1/866.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Downloading ijson-3.4.0.post0-cp312-cp312-macosx_11_0_arm64.whl (59 kB)\n",
      "Downloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
      "Downloading lxml-6.0.2-cp312-cp312-macosx_10_13_universal2.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lz4-4.4.4-cp312-cp312-macosx_11_0_arm64.whl (189 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
      "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: cbor, warc3-wet-clueweb09, zlib-state\n",
      "  Building wheel for cbor (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cbor: filename=cbor-1.0.0-cp312-cp312-macosx_14_0_arm64.whl size=20300 sha256=e778be9d3f718c0f09877a2d2eb7c34d36eb1b879d0e9405865c13f01d344399\n",
      "  Stored in directory: /Users/uvuv/Library/Caches/pip/wheels/44/3e/21/a739cbcc331a1ab45c326d6edbdac6118de4402f6076e30ff1\n",
      "  Building wheel for warc3-wet-clueweb09 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18997 sha256=88c982a517e8e1e8394a470ee12efd7a8cfa0b6acc95c69751c6d2e8bd88b4b9\n",
      "  Stored in directory: /Users/uvuv/Library/Caches/pip/wheels/f6/85/c2/9f0f621def52a1d5db7d29984f81e45f9fb6dfeb1a4eb6e31c\n",
      "  Building wheel for zlib-state (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for zlib-state: filename=zlib_state-0.1.10-cp312-cp312-macosx_14_0_arm64.whl size=10246 sha256=6e465e748602fff9195d525658d0427638c214a1c61e2dd8a5f6b446415d272e\n",
      "  Stored in directory: /Users/uvuv/Library/Caches/pip/wheels/40/80/47/fcc42495f3f4d4a090121d2517857686dd9c9482610ca5af65\n",
      "Successfully built cbor warc3-wet-clueweb09 zlib-state\n",
      "Installing collected packages: warc3-wet-clueweb09, warc3-wet, cbor, zlib-state, unlzw3, trec-car-tools, soupsieve, lz4, lxml, ijson, inscriptis, beautifulsoup4, ir_datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [ir_datasets]\u001b[0m [ir_datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed beautifulsoup4-4.14.2 cbor-1.0.0 ijson-3.4.0.post0 inscriptis-2.6.0 ir_datasets-0.5.11 lxml-6.0.2 lz4-4.4.4 soupsieve-2.8 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.10\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ir_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "12cf9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: msmarco-passage/dev\n",
      "Has queries: True\n",
      "Has docs: True\n",
      "Has qrels: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Please confirm you agree to the MSMARCO data usage agreement found at <http://www.msmarco.org/dataset.aspx>\n",
      "[INFO] If you have a local copy of https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz, you can symlink it here to avoid downloading it again: /Users/uvuv/.ir_datasets/downloads/c177b2795d5f2dcc524cf00fcd973be1\n",
      "[INFO] [starting] https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz\n",
      "[INFO] [finished] https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz: [00:03] [18.9MB] [6.06MB/s]\n",
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries: 101093\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ir_datasets.load(\"msmarco-passage/dev\")\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(f\"Dataset name: {dataset.dataset_id()}\")\n",
    "print(f\"Has queries: {dataset.has_queries()}\")\n",
    "print(f\"Has docs: {dataset.has_docs()}\")\n",
    "print(f\"Has qrels: {dataset.has_qrels()}\")\n",
    "\n",
    "# Count the queries\n",
    "queries_count = sum(1 for _ in dataset.queries_iter())\n",
    "print(f\"Number of queries: {queries_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc28bd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== First 5 queries ===\n",
      "Query ID: 1048578, Text: cost of endless pools/swim spa\n",
      "Query ID: 1048579, Text: what is pcnt\n",
      "Query ID: 1048580, Text: what is pcb waste\n",
      "Query ID: 1048581, Text: what is pbis?\n",
      "Query ID: 1048582, Text: what is paysky\n"
     ]
    }
   ],
   "source": [
    "# Iterate through queries\n",
    "print(\"\\n=== First 5 queries ===\")\n",
    "for i, query in enumerate(dataset.queries_iter()):\n",
    "    if i >= 5:  # Show only first 5\n",
    "        break\n",
    "    print(f\"Query ID: {query.query_id}, Text: {query.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3f5b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Примеры релевантных пар query-passage ===\n",
      "Query 1102432 -> Passage 2026790, Relevance: 1\n",
      "Query 1102431 -> Passage 7066866, Relevance: 1\n",
      "Query 1102431 -> Passage 7066867, Relevance: 1\n",
      "Query 1090282 -> Passage 7066900, Relevance: 1\n",
      "Query 39449 -> Passage 7066905, Relevance: 1\n",
      "Query 76162 -> Passage 7066915, Relevance: 1\n",
      "Query 195512 -> Passage 7066971, Relevance: 1\n",
      "Query 1090280 -> Passage 7067004, Relevance: 1\n",
      "Query 331318 -> Passage 5309290, Relevance: 1\n",
      "Query 300674 -> Passage 7067032, Relevance: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Примеры релевантных пар query-passage ===\")\n",
    "for i, qrel in enumerate(dataset.qrels_iter()):\n",
    "    if i >= 10:  # Покажем первые 10\n",
    "        break\n",
    "    print(f\"Query {qrel.query_id} -> Passage {qrel.doc_id}, Relevance: {qrel.relevance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Примеры релевантных пар query-passage ===\")\n",
    "for i, qrel in enumerate(dataset.qrels_iter()):\n",
    "    if i >= 10:  # Покажем первые 10\n",
    "        break\n",
    "    print(f\"Query {qrel.query_id} -> Passage {qrel.doc_id}, Relevance: {qrel.relevance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_mapping = {}\n",
    "for qrel in dataset.qrels_iter():\n",
    "    qrels_mapping.setdefault(qrel.query_id, {})[qrel.doc_id] = qrel.relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c48a840c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uvuv/Informational_Search/spladev2/cloned/splade/splade/models/transformer_rep.py:89: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if self.fp16 else NullContextManager():\n",
      "/Users/uvuv/Informational_Search/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/uvuv/Informational_Search/spladev2/cloned/splade/splade/models/transformer_rep.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if self.fp16 else NullContextManager():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'8003157': 2.2332911491394043,\n",
       " '8003158': 4.466582298278809,\n",
       " '8003159': 6.699873447418213}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = {\n",
    "    \"endless\": [(\"8003157\", 1), (\"8003158\", 2), (\"8003159\", 3)]\n",
    "}\n",
    "\n",
    "for query in dataset.queries_iter():\n",
    "    query_id = query.query_id\n",
    "    relevant_passages = qrels_mapping.get(query_id, {})\n",
    "    \n",
    "    if len(relevant_passages) > 0:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            query_rep = model(d_kwargs=tokenizer(query.text, return_tensors=\"pt\"))[\"d_rep\"].squeeze()\n",
    "\n",
    "        col = torch.nonzero(query_rep).squeeze().cpu().tolist()\n",
    "        weights = query_rep[col].cpu().tolist()\n",
    "        d = {k: v for k, v in zip(col, weights)}\n",
    "        sorted_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "        bow_rep = []\n",
    "        for k, v in sorted_d.items():\n",
    "            bow_rep.append((reverse_voc[k], v))\n",
    "        \n",
    "        g_d = {}\n",
    "        for q, v in bow_rep:\n",
    "            if q in dd:\n",
    "                for doc_id, doc_relevance in dd[q]:\n",
    "                    g_d[doc_id] = g_d.get(doc_id, 0) + doc_relevance * v\n",
    "\n",
    "        break\n",
    "\n",
    "        # relevant_indicies = torch.argsort(query_rep @ doc_rep.T, descending=True)\n",
    "        # result_index = torch.argmax(torch.tensor(df['train']['passages'][index]['is_selected']))\n",
    "        # rank = torch.where(relevant_indicies == result_index)\n",
    "        # mrr10 += 1 / (rank[0] + 1)\n",
    "\n",
    "        # print(relevant_passages)\n",
    "\n",
    "g_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b49a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
