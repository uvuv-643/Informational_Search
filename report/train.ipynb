{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import ir_datasets\n",
    "\n",
    "class MSMARCODataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length=128, max_items=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        dataset = ir_datasets.load(\"msmarco-passage/train/triples-small\")\n",
    "        \n",
    "        queries = {}\n",
    "        docs = {}\n",
    "        for q in dataset.queries_iter():\n",
    "            queries[q.query_id] = q.text\n",
    "        for d in dataset.docs_iter():\n",
    "            docs[d.doc_id] = d.text\n",
    "        \n",
    "        self.data = []\n",
    "        count = 0\n",
    "        for item in dataset.docpairs_iter():\n",
    "            if max_items and count >= max_items:\n",
    "                break\n",
    "            self.data.append({\n",
    "                'query': queries[item.query_id],\n",
    "                'pos_doc': docs[item.doc_id_a],\n",
    "                'neg_doc': docs[item.doc_id_b]\n",
    "            })\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        query = self.tokenizer(item['query'], truncation=True, padding='max_length', \n",
    "                               max_length=self.max_length, return_tensors='pt')\n",
    "        pos_doc = self.tokenizer(item['pos_doc'], truncation=True, padding='max_length',\n",
    "                                 max_length=self.max_length, return_tensors='pt')\n",
    "        neg_doc = self.tokenizer(item['neg_doc'], truncation=True, padding='max_length',\n",
    "                                 max_length=self.max_length, return_tensors='pt')\n",
    "        return {\n",
    "            'q_ids': query['input_ids'].squeeze(),\n",
    "            'q_mask': query['attention_mask'].squeeze(),\n",
    "            'p_ids': pos_doc['input_ids'].squeeze(),\n",
    "            'p_mask': pos_doc['attention_mask'].squeeze(),\n",
    "            'n_ids': neg_doc['input_ids'].squeeze(),\n",
    "            'n_mask': neg_doc['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "class SPLADE(nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.bert_mlm = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert_mlm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        relu_logits = F.relu(logits)\n",
    "        relu_logits = relu_logits * attention_mask.unsqueeze(-1)\n",
    "        pooled, _ = torch.max(relu_logits, dim=1)\n",
    "        pooled = torch.clamp(pooled, max=10.0)\n",
    "        return torch.log1p(pooled + 1e-8)\n",
    "\n",
    "def compute_loss(q_reps, p_reps, n_reps, lambda_q=1e-3, lambda_d=1e-4, tau=0.1):\n",
    "    batch_size = q_reps.size(0)\n",
    "    all_docs = torch.cat([p_reps, n_reps], dim=0)\n",
    "    \n",
    "    q_reps = F.normalize(q_reps, p=2, dim=-1)\n",
    "    all_docs = F.normalize(all_docs, p=2, dim=-1)\n",
    "    \n",
    "    scores = torch.matmul(q_reps, all_docs.T) / tau\n",
    "    scores = torch.clamp(scores, min=-100, max=100)\n",
    "    \n",
    "    labels = torch.arange(batch_size, device=q_reps.device)\n",
    "    ce_loss = F.cross_entropy(scores, labels)\n",
    "    \n",
    "    l1_q = lambda_q * torch.mean(torch.abs(q_reps))\n",
    "    l1_d = lambda_d * torch.mean(torch.abs(all_docs))\n",
    "    \n",
    "    total_loss = ce_loss + l1_q + l1_d\n",
    "    \n",
    "    if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "        return torch.tensor(0.0, device=q_reps.device, requires_grad=True)\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "parser = argparse.ArgumentParser(prog='splade_training')\n",
    "parser.add_argument('-r', '--result', required=True, help='Output file')\n",
    "\n",
    "\n",
    "if name == '__main__':\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    print('Loading dataset...')\n",
    "    dataset = MSMARCODataset(tokenizer, max_items=3 * 32000)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    print('Training set has {} instances'.format(len(dataset)))\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SPLADE().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(1):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        total_loss = 0.0\n",
    "        running = 0.0\n",
    "        valid_steps = 0\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            try:\n",
    "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                    q_reps = model(batch['q_ids'].to(device), batch['q_mask'].to(device))\n",
    "                    p_reps = model(batch['p_ids'].to(device), batch['p_mask'].to(device))\n",
    "                    n_reps = model(batch['n_ids'].to(device), batch['n_mask'].to(device))\n",
    "                    loss = compute_loss(q_reps, p_reps, n_reps)\n",
    "                \n",
    "                if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    \n",
    "                    loss_val = loss.item()\n",
    "                    total_loss += loss_val\n",
    "                    running += loss_val\n",
    "                    valid_steps += 1\n",
    "                else:\n",
    "                    scaler.update()\n",
    "                    continue\n",
    "                    \n",
    "            except RuntimeError as e:\n",
    "                scaler.update()\n",
    "                continue\n",
    "            \n",
    "            if (i + 1) % 100 == 0 and valid_steps > 0:\n",
    "                print('  batch {} loss: {}'.format(i + 1, running/valid_steps))\n",
    "                running = 0.0\n",
    "                valid_steps = 0\n",
    "        \n",
    "        if len(dataloader) > 0:\n",
    "            print('Epoch {} avg loss = {}'.format(epoch + 1, total_loss / len(dataloader)))\n",
    "    \n",
    "    model.eval()\n",
    "    test_queries = [\n",
    "        \"what is python programming\",\n",
    "        \"how to lose weight\",\n",
    "        \"best restaurants in paris\",\n",
    "        \"covid vaccine side effects\",\n",
    "        \"machine learning tutorial\",\n",
    "        \"climate change causes\",\n",
    "        \"how to cook pasta\",\n",
    "        \"bitcoin price prediction\",\n",
    "        \"yoga benefits health\",\n",
    "        \"electric cars pros cons\"\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for query in test_queries:\n",
    "            tokens = tokenizer(query, return_tensors='pt', padding=True, truncation=True)\n",
    "            q_rep = model(tokens['input_ids'].to(device), tokens['attention_mask'].to(device))\n",
    "            \n",
    "            top_indices = torch.topk(q_rep[0], k=20).indices\n",
    "            top_weights = torch.topk(q_rep[0], k=20).values\n",
    "            \n",
    "            print('\\nQuery: {}'.format(query))\n",
    "            print('Top tokens: {}'.format([(tokenizer.decode([idx]), '{:.2f}'.format(weight.item())) \n",
    "                                          for idx, weight in zip(top_indices, top_weights)]))\n",
    "    \n",
    "    model_path = 'splade_checkpoint.pt'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    print('Move {} to {}'.format(model_path, args.result))\n",
    "    shutil.move(model_path, args.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01341e",
   "metadata": {},
   "source": [
    "![Learning loss](docs/loss_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2286a3",
   "metadata": {},
   "source": [
    "## Query encoding example:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \n",
    "    \"what is python programming\": [('python', '1.97'), ('software', '0.78'), ('programming', '0.59'), ('language', '0.53'), ('logic', '0.44'), ('languages', '0.30'), ('technology', '0.24'), ('computer', '0.19'), ('c', '0.07'), ('[unused9]', '0.00'), ('[unused6]', '0.00'), ('[unused8]', '0.00'), ('[unused4]', '0.00'), ('[unused2]', '0.00'), ('[unused5]', '0.00'), ('[unused7]', '0.00'), ('[unused3]', '0.00'), ('[unused1]', '0.00'), ('[PAD]', '0.00'), ('[unused0]', '0.00')],\n",
    "\n",
    "    \"how to lose weight\": [('loss', '1.29'), ('exercise', '1.26'), ('weight', '1.14'), ('yoga', '0.93'), ('diet', '0.89'), ('washing', '0.70'), ('lose', '0.49'), ('eating', '0.42'), ('daily', '0.32'), ('pills', '0.24'), ('change', '0.21'), ('losing', '0.19'), ('gain', '0.17'), ('training', '0.16'), ('body', '0.12'), ('[unused3]', '0.00'), ('[PAD]', '0.00'), ('[unused2]', '0.00'), ('[unused0]', '0.00'), ('[unused1]', '0.00')],\n",
    "\n",
    "    \"best restaurants in paris\": [('restaurant', '1.48'), ('paris', '1.47'), ('hotel', '1.38'), ('france', '1.20'), ('french', '0.79'), ('food', '0.75'), ('restaurants', '0.71'), ('hotels', '0.58'), ('resort', '0.55'), ('club', '0.33'), ('europe', '0.29'), ('museums', '0.26'), ('store', '0.23'), ('town', '0.08'), ('shop', '0.07'), ('spain', '0.05'), ('[unused2]', '0.00'), ('[unused1]', '0.00'), ('[PAD]', '0.00'), ('[unused0]', '0.00')]\n",
    "\n",
    "    \"covid vaccine side effects\": [('##vid', '1.99'), ('effects', '1.57'), ('co', '1.43'), ('nausea', '1.29'), ('poisoning', '1.25'), ('vaccine', '1.17'), ('headache', '0.60'), ('effect', '0.54'), ('pills', '0.44'), ('influenza', '0.17'), ('virus', '0.08'), ('[unused7]', '0.00'), ('[unused3]', '0.00'), ('[unused6]', '0.00'), ('[unused4]', '0.00'), ('[unused5]', '0.00'), ('[unused1]', '0.00'), ('[unused0]', '0.00'), ('[unused2]', '0.00'), ('[PAD]', '0.00')],\n",
    "\n",
    "    \"machine learning tutorial\": [('machine', '1.72'), ('learning', '1.18'), ('education', '0.72'), ('training', '0.65'), ('skills', '0.64'), ('technology', '0.58'), ('teaching', '0.57'), ('language', '0.28'), ('software', '0.23'), ('instruction', '0.18'), ('science', '0.17'), ('theory', '0.11'), ('method', '0.11'), ('cognitive', '0.08'), ('equipment', '0.05'), ('school', '0.04'), ('degree', '0.03'), ('process', '0.02'), ('[unused0]', '0.00'), ('[PAD]', '0.00')],\n",
    "\n",
    "    \"climate change causes\": [('climate', '1.69'), ('cause', '1.29'), ('impact', '1.19'), ('causes', '1.12'), ('warming', '1.04'), ('effect', '0.91'), ('change', '0.89'), ('causing', '0.88'), ('earth', '0.87'), ('effects', '0.74'), ('pollution', '0.71'), ('environment', '0.62'), ('increase', '0.58'), ('emissions', '0.58'), ('economy', '0.51'), ('global', '0.48'), ('cold', '0.29'), ('atmosphere', '0.24'), ('factors', '0.21'), ('source', '0.19')]\n",
    "\n",
    "    \"how to cook pasta\": [('pasta', '1.80'), ('cooking', '0.89'), ('baking', '0.71'), ('cook', '0.69'), ('hours', '0.41'), ('boiling', '0.38'), ('oven', '0.24'), ('heat', '0.16'), ('[unused10]', '0.00'), ('[unused9]', '0.00'), ('[unused7]', '0.00'), ('[unused8]', '0.00'), ('[unused4]', '0.00'), ('[unused3]', '0.00'), ('[unused5]', '0.00'), ('[unused6]', '0.00'), ('[unused2]', '0.00'), ('[unused1]', '0.00'), ('[PAD]', '0.00'), ('[unused0]', '0.00')],\n",
    "    \n",
    "    \"bitcoin price prediction\": [('price', '1.34'), ('prediction', '1.25'), ('bit', '1.22'), ('stock', '1.21'), ('##co', '0.98'), ('value', '0.75'), ('percentage', '0.49'), ('bank', '0.40'), ('increase', '0.40'), ('rate', '0.19'), ('market', '0.18'), ('fortune', '0.17'), ('cash', '0.13'), ('accounting', '0.06'), ('[unused4]', '0.00'), ('[unused3]', '0.00'), ('[PAD]', '0.00'), ('[unused0]', '0.00'), ('[unused1]', '0.00'), ('[unused2]', '0.00')]\n",
    "\n",
    "    \"yoga benefits health\": [('yoga', '2.03'), ('benefits', '0.97'), ('cure', '0.76'), ('exercise', '0.63'), ('healing', '0.57'), ('health', '0.44'), ('meditation', '0.37'), ('body', '0.33'), ('training', '0.11'), ('wellness', '0.06'), ('diet', '0.06'), ('mental', '0.06'), ('therapy', '0.05'), ('breathing', '0.01'), ('[unused4]', '0.00'), ('[unused3]', '0.00'), ('[PAD]', '0.00'), ('[unused0]', '0.00'), ('[unused1]', '0.00'), ('[unused2]', '0.00')],\n",
    "\n",
    "    \"electric cars pros cons\": [('electric', '1.49'), ('vehicle', '1.12'), ('car', '0.81'), ('pro', '0.73'), ('con', '0.62'), ('engine', '0.46'), ('bmw', '0.42'), ('toyota', '0.30'), ('tesla', '0.25'), ('ford', '0.23'), ('power', '0.19'), ('driving', '0.17'), ('gear', '0.11'), ('fuel', '0.05'), ('ev', '0.04'), ('[unused3]', '0.00'), ('[PAD]', '0.00'), ('[unused2]', '0.00'), ('[unused0]', '0.00'), ('[unused1]', '0.00')]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a79ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
