{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36227e2f",
   "metadata": {},
   "source": [
    "### Загружаются различные модели: \n",
    "\n",
    "```python\n",
    "    model = Splade(model_type_or_dir, agg=\"max\")    # предобученный splade с библиотеки\n",
    "\n",
    "    model = SPLADE().to(device)                     # самописная обученная версия splade \n",
    "    model.load_state_dict(torch.load('model.pt'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542454ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "df = load_dataset(\"microsoft/ms_marco\", \"v1.1\")\n",
    "\n",
    "model_type_or_dir = \"naver/splade-cocondenser-ensembledistil\"\n",
    "\n",
    "# model = Splade(model_type_or_dir, agg=\"max\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SPLADE(nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.bert_mlm = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert_mlm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        relu_logits = F.relu(logits)\n",
    "        relu_logits = relu_logits * attention_mask.unsqueeze(-1)\n",
    "        pooled, _ = torch.max(relu_logits, dim=1)\n",
    "        weights = torch.log1p(pooled)\n",
    "        return weights\n",
    "    \n",
    "model = SPLADE().to(device)\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type_or_dir)\n",
    "reverse_voc = {v: k for k, v in tokenizer.vocab.items()}\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b86c96",
   "metadata": {},
   "source": [
    "### Запускаю N инстансов блокнота, каждый из которых обрабатывает часть датасета для обратного индекса\n",
    "\n",
    "```python\n",
    "end_position = 9_000_000    # всего документов в датасете\n",
    "sline_num = 0               # номер текущего инстанса -- только это поле нужно менять\n",
    "slice_cnt = 3               # общее количество инстансов\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28653e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "reverse_index = defaultdict(list)\n",
    "batch_size = 32\n",
    "\n",
    "last_save_time = time.time()\n",
    "save_interval = 30 * 600\n",
    "counter = 0\n",
    "\n",
    "start_position = 0\n",
    "end_position = 9_000_000\n",
    "sline_num = 0\n",
    "slice_cnt = 3\n",
    "actual_start = (end_position - start_position) // slice_cnt * sline_num\n",
    "actual_end = (end_position - start_position) // slice_cnt * (sline_num + 1)\n",
    "\n",
    "if not os.path.exists(\"backups\"):\n",
    "    os.makedirs(\"backups\")\n",
    "\n",
    "with open(\"collection.tsv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    \n",
    "    batch_docs = []\n",
    "    batch_ids = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    for row in tqdm(rd):\n",
    "        counter += 1\n",
    "        if counter < actual_start:\n",
    "            continue\n",
    "            \n",
    "        batch_ids.append(row[0])\n",
    "        batch_docs.append(row[1])\n",
    "        \n",
    "        if len(batch_docs) == batch_size:\n",
    "            passage_tokens = tokenizer(batch_docs, return_tensors=\"pt\", truncation=True, \n",
    "                                      max_length=512, padding=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_ids = passage_tokens['input_ids'].to(device)\n",
    "                attention_mask = passage_tokens['attention_mask'].to(device)\n",
    "                batch_reps = model(input_ids, attention_mask)\n",
    "            \n",
    "            for i, (doc_id, doc_rep) in enumerate(zip(batch_ids, batch_reps)):\n",
    "                doc_rep = doc_rep.squeeze()\n",
    "                mask = doc_rep > 0.01\n",
    "                indices = torch.arange(doc_rep.size(0), device=device)[mask]\n",
    "                weights = doc_rep[mask]\n",
    "                \n",
    "                sorted_indices = weights.argsort(descending=True)\n",
    "                indices = indices[sorted_indices].cpu().numpy()\n",
    "                weights = weights[sorted_indices].cpu().numpy()\n",
    "                \n",
    "                for idx, weight in zip(indices, weights):\n",
    "                    reverse_index[reverse_voc[idx]].append((doc_id, float(weight)))\n",
    "            \n",
    "            total_processed += len(batch_docs)\n",
    "            batch_docs = []\n",
    "            batch_ids = []\n",
    "            \n",
    "            current_time = time.time()\n",
    "            if current_time - last_save_time >= save_interval or counter > actual_end:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                \n",
    "                with open(f\"backups/reverse_index_{timestamp}.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(dict(reverse_index), f)\n",
    "                \n",
    "                with open(f\"backups/progress_{timestamp}.txt\", \"w\") as f:\n",
    "                    f.write(f\"Documents processed: {total_processed}\\n\")\n",
    "                    f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                \n",
    "                print(f\"\\nBackup saved at {timestamp} - {total_processed} documents processed\")\n",
    "                last_save_time = current_time\n",
    "                \n",
    "                if counter > actual_end:\n",
    "                    break\n",
    "    \n",
    "    \n",
    "print(f\"\\nProcessing complete. Total documents processed: {total_processed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f52bec",
   "metadata": {},
   "source": [
    "### После инференса всех частей выполняю merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3513838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "result = {}\n",
    "\n",
    "def merge_pickles(file_paths, output_path):\n",
    "    global result\n",
    "    for file_path in tqdm(file_paths):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            current = pickle.load(f)\n",
    "        \n",
    "        for key, value in tqdm(current.items()):\n",
    "            if key in result:\n",
    "                result[key].extend(value)\n",
    "            else:\n",
    "                result[key] = value\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "files = [\n",
    "    '/home/jupyter/datasphere/project/backups/reverse_index_20251123_161520.pkl',\n",
    "    '/home/jupyter/datasphere/project/backups/reverse_index_20251123_174727.pkl',\n",
    "    '/home/jupyter/datasphere/project/backups/reverse_index_20251123_174815.pkl'\n",
    "]\n",
    "merge_pickles(files, 'merged.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
